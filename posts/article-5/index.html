<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Apprendre avec les LLM dans l'UI/UX | Marc Bouriot</title><link rel=icon href=/favicon.svg sizes=any type=image/svg+xml><meta property="og:url" content="https://kemxr.github.io/labveilletech/posts/article-5/"><meta property="og:site_name" content="Marc Bouriot"><meta property="og:title" content="Apprendre avec les LLM dans l'UI/UX"><meta property="og:description" content="Dans ma veille sur les LLM appliqués à l’UI/UX, j’ai découvert comment ces modèles transforment les interfaces en systèmes vivants capables de s’adapter en temps réel. Mais au-delà de leur usage dans les produits finaux, ils représentent aussi un outil exceptionnel pour apprendre mon domaine. L’article étudié explique parfaitement comment utiliser les LLM comme un atelier de prototypage rapide qui accélère la maîtrise des concepts UX.
La première découverte clé est que les interfaces générées par LLM obligent à penser en systèmes plutôt qu’en écrans figés. En décrivant un besoin en langage naturel (« un dashboard gaming avec HUD minimaliste et stats adaptatives »), le modèle assemble automatiquement les composants. Cela m’apprend à structurer ma pensée de designer : quelles sont les règles essentielles, les contraintes d’accessibilité, les priorités hiérarchiques ? Chaque prototype généré devient un exercice pratique pour analyser ce qui fonctionne ou non en termes de charge cognitive et de lisibilité."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-24T23:00:00-03:00"><meta property="article:modified_time" content="2025-03-24T23:00:00-03:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Apprendre avec les LLM dans l'UI/UX"><meta name=twitter:description content="Dans ma veille sur les LLM appliqués à l’UI/UX, j’ai découvert comment ces modèles transforment les interfaces en systèmes vivants capables de s’adapter en temps réel. Mais au-delà de leur usage dans les produits finaux, ils représentent aussi un outil exceptionnel pour apprendre mon domaine. L’article étudié explique parfaitement comment utiliser les LLM comme un atelier de prototypage rapide qui accélère la maîtrise des concepts UX.
La première découverte clé est que les interfaces générées par LLM obligent à penser en systèmes plutôt qu’en écrans figés. En décrivant un besoin en langage naturel (« un dashboard gaming avec HUD minimaliste et stats adaptatives »), le modèle assemble automatiquement les composants. Cela m’apprend à structurer ma pensée de designer : quelles sont les règles essentielles, les contraintes d’accessibilité, les priorités hiérarchiques ? Chaque prototype généré devient un exercice pratique pour analyser ce qui fonctionne ou non en termes de charge cognitive et de lisibilité."><link rel=stylesheet href=/labveilletech/css/root.min.0e732b812b9751962e01a7c4798a1211cd5f8ac8abec7f99793fe306989e459f.css integrity="sha256-DnMrgSuXUZYuAafEeYoSEc1fisir7H+ZeT/jBpieRZ8=" crossorigin=anonymous><link rel=stylesheet href=/labveilletech/css/bundle.min.35b15f72eff61cbb106d05bab1ea1aebb778d0ba2861c763af0af0d80f43db4f.css integrity="sha256-NbFfcu/2HLsQbQW6seoa67d40LooYcdjrwrw2A9D208=" crossorigin=anonymous><script src=/labveilletech/js/bundle.cc8ae9952dbfb731affafabdf26e5c60a6910047ff59ccdeaf1daebaa26c8830.js integrity="sha256-zIrplS2/tzGv+vq98m5cYKaRAEf/Wczerx2uuqJsiDA=" crossorigin=anonymous></script><script defer src=/labveilletech/js/search/flexsearch.compact.3928d3d2172bc0b5f75d9458fb9255f0befce1c7239ba6e5d2c58ed4b82c2073.js integrity="sha256-OSjT0hcrwLX3XZRY+5JV8L784ccjm6bl0sWO1LgsIHM="></script><script defer src=/labveilletech/js/search/search.7ddc6699e16c4c233f5fb54102a1a2606d902351faf1b6f9a08a0cc64e6ba745.js integrity="sha256-fdxmmeFsTCM/X7VBAqGiYG2QI1H68bb5oIoMxk5rp0U="></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel=stylesheet></head><body class=notransition><div id=container><header id=main-header><div role=navigation aria-label=Main><div class=nav-left><a href=https://kemxr.github.io/labveilletech/ style=color:inherit>Marc Bouriot</a></div><div class=nav-right><div style=position:absolute;width:0;height:0><div id=nav-dropdown-menu class=hidden href=#><div class=nav-item><a aria-current=true class=ancestor href=/labveilletech/posts/>Posts</a></div><div class=nav-item><a href=/labveilletech/about/>About</a></div></div></div><a id=nav-dropdown-button href=#><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></a><div id=nav-menu><div class=nav-item><a aria-current=true class=ancestor href=/labveilletech/posts/>Posts</a></div><div class=nav-item><a href=/labveilletech/about/>About</a></div></div><a id=theme-switcher href=#><svg class="light-icon" viewBox="0 0 24 24" fill="none"><path d="M12 3V4m0 16v1M4 12H3M6.31412 6.31412 5.5 5.5m12.1859.81412L18.5 5.5M6.31412 17.69 5.5 18.5001M17.6859 17.69 18.5 18.5001M21 12H20m-4 0c0 2.2091-1.7909 4-4 4-2.20914.0-4-1.7909-4-4 0-2.20914 1.79086-4 4-4 2.2091.0 4 1.79086 4 4z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg>
<svg class="dark-icon" viewBox="0 0 24 24" fill="none"><path d="M3.32031 11.6835c0 4.9706 4.02944 9 8.99999 9 3.7872.0 7.028-2.3392 8.3565-5.6515C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834c-4.9706.0-8.99999-4.0294-8.99999-8.99998C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996 5.65605 4.66028 3.32031 7.89912 3.32031 11.6835z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></a></div></div></header><div class="flex grow"><div id=main-pane><main id=main-content><div class=single-header><ol class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=https://kemxr.github.io/labveilletech/><span itemprop=name>Home</span>
</a><meta itemprop=position content='1'></li><span>&nbsp»&nbsp</span><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=https://kemxr.github.io/labveilletech/posts/><span itemprop=name>Posts</span>
</a><meta itemprop=position content='2'></li><span>&nbsp»&nbsp</span></ol><h1>Apprendre avec les LLM dans l'UI/UX</h1><time class=dim datetime=2025-03-24T23:00:00-03:00>March 24, 2025</time><div class=term-container><div class=tag><a href=https://kemxr.github.io/labveilletech/tags/ai/>#AI</a></div></ol></div><section class=page-section><p>Dans ma veille sur les LLM appliqués à l&rsquo;UI/UX, j&rsquo;ai découvert comment ces modèles transforment les interfaces en systèmes vivants capables de s&rsquo;adapter en temps réel. Mais au-delà de leur usage dans les produits finaux, ils représentent aussi un outil exceptionnel pour apprendre mon domaine. L&rsquo;article étudié explique parfaitement comment utiliser les LLM comme un atelier de prototypage rapide qui accélère la maîtrise des concepts UX.</p><p>La première découverte clé est que les interfaces générées par LLM obligent à penser en systèmes plutôt qu&rsquo;en écrans figés. En décrivant un besoin en langage naturel (« un dashboard gaming avec HUD minimaliste et stats adaptatives »), le modèle assemble automatiquement les composants. Cela m&rsquo;apprend à structurer ma pensée de designer : quelles sont les règles essentielles, les contraintes d&rsquo;accessibilité, les priorités hiérarchiques ? Chaque prototype généré devient un exercice pratique pour analyser ce qui fonctionne ou non en termes de charge cognitive et de lisibilité.</p><p>Ensuite, les outils mentionnés (Streamlit, Gradio, Chainlit) montrent comment passer du prompt à l&rsquo;interface en quelques minutes. Pour l&rsquo;apprentissage, c&rsquo;est révolutionnaire : au lieu de passer des heures à maquetter manuellement, je peux tester dix variantes d&rsquo;un même écran en changeant juste la formulation. Cela me permet de me concentrer sur l&rsquo;analyse critique – parcours utilisateur, affordances, cohérence visuelle – plutôt que sur la technique pure. L&rsquo;article insiste sur le fait que les designers deviennent des curateurs : ils définissent les garde-fous (composants autorisés, règles WCAG, style guide) et laissent l&rsquo;IA composer dans ce cadre. Apprendre à formuler ces contraintes fait partie de la nouvelle compétence essentielle.</p><p>Une autre idée marquante est le passage du code à la conversation. Dans les analytics modernes, on demande simplement « montre les ventes par région » et l&rsquo;IA génère le graphique adapté. Pour mon apprentissage UX/UI, cela signifie m&rsquo;entraîner à dialoguer avec l&rsquo;interface : décrire un scénario utilisateur, voir la proposition, demander des ajustements ciblés (« simplifie ce menu de navigation »). Si le résultat est confus, c&rsquo;est que ma description l&rsquo;était aussi. Le LLM devient un miroir de la clarté de ma pensée de designer.</p><p>Cependant il y a aussi des limites cruciales : hallucinations de l&rsquo;IA, incohérences de parcours, risques éthiques. C&rsquo;est parfait pour pratiquer l&rsquo;esprit critique : repérer ce qui viole les heuristiques Nielsen dans une interface générée, identifier les problèmes d&rsquo;accessibilité, ajuster le prompt pour corriger. Le principe « human-in-the-loop » devient un exercice concret : l&rsquo;IA propose, je juge, je teste, je reste responsable. Chaque écran généré est une mini-revue UX pratique.</p><p>Au final, les LLM accélèrent mon apprentissage en permettant de prototyper instantanément, d&rsquo;explorer des dizaines de variantes, de travailler ma capacité à formuler des besoins précis et de réviser les fondamentaux à chaque itération. Ils ne remplacent pas l&rsquo;humain, mais transforment l&rsquo;apprenant en superviseur intelligent d&rsquo;un atelier créatif sans limites.</p><p>Cet article m&rsquo;intéresse car il montre comment les LLM génèrent des interfaces dynamiques en temps réel à partir de prompts naturels, transformant l&rsquo;UX/UI en systèmes adaptatifs. Pour mon domaine (interfaces gaming claires/immersives), c&rsquo;est utile dès maintenant pour prototyper vite (10 variantes d&rsquo;HUD en 5 minutes) et plus tard en discovery/client pour valider des idées instantanément. Ça me positionne comme curateur intelligent : je définis les règles d&rsquo;accessibilité/composants, l&rsquo;IA assemble. Avantage compétitif pour adapter des milliers de profils joueurs.</p><p>Source : <a href=https://www.linkedin.com/pulse/ux-design-without-designers-how-llms-rewriting-ui-real-tabor-x8gle/>https://www.linkedin.com/pulse/ux-design-without-designers-how-llms-rewriting-ui-real-tabor-x8gle/</a></p></section></main><footer id=main-footer><div class=footer><a href=#>Scroll to Top</a><div class=footer-copyright><div class=dim>© 2026 Marc Bouriot</div><div>Made with ❤️ and powered by <a href=https://github.com/math-queiroz/rusty-typewriter target=_blank>Rusty Typewriter</a> theme for <a href=https://gohugo.io/ target=_blank>Hugo</a></div></div></div></footer></div><aside id=side-pane class=side-sticky><div class=side-details><span>548 words</span>
<span>3 - 4 minutes read</span><div class=side-details-taxonomy><small>topics:
<span class=details-taxonomy><a href=https://kemxr.github.io/labveilletech/topics/tendance>Tendance</a></span></small></div></div></aside></div></div></body></html>